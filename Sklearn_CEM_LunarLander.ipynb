{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super-deep reinforcement learning with scikit-learn\n",
    "\n",
    "Which environment to choose for deep reinforcement learning experiments?\n",
    "Theano or Tensorflow? GPU or MPI? \n",
    "\n",
    "Who the hack needs any of these? Now you have Scikit-Learn!\n",
    "\n",
    "Jokes aside, this demo shows the awesome scikit-learn deep reinforcement learning agent trained with __crossentropy method__ _(because TD is for sissies)_.\n",
    "\n",
    "Requires: gym, scikit-learn (with MLPClassifier), numpy, joblib\n",
    "\n",
    "Also this demo utilizes all CPU cores you have. To change that, tweak n_jobs param.\n",
    "\n",
    "Read more about crossentropy method [in general](https://people.smp.uq.edu.au/DirkKroese/ps/aortut.pdf), [for rl](https://people.smp.uq.edu.au/DirkKroese/ps/eormsCE.pdf), [for rl again](https://esc.fnwi.uva.nl/thesis/centraal/files/f2110275396.pdf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DISPLAY=:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "import gym, gym.wrappers\n",
    "gym.logger.level=0 #gym.youre(\"drunk\").shut_up()\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#Create environment\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "\n",
    "#Create agent\n",
    "agent = MLPClassifier(hidden_layer_sizes=(256,512),\n",
    "                      activation='tanh',\n",
    "                      solver='adam',\n",
    "                      warm_start=True,max_iter=1\n",
    "                     )\n",
    "#initialize agent by feeding it with some random bullshit\n",
    "agent.fit([env.reset()]*n_actions,range(n_actions));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "def generate_session(t_max=10**3):\n",
    "    \"\"\"\n",
    "    Just ask agent to predict action and see how env reacts - repeat until exhaustion.\n",
    "    :param t_max: after this many steps the session is forcibly stopped. MAKE SURE IT'S ENOUGH!\"\"\"\n",
    "    states,actions,total_reward = [],[],0\n",
    "    \n",
    "    s = env.reset()    \n",
    "    for t in count():\n",
    "        a = np.random.choice(n_actions,p=agent.predict_proba([s])[0])\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        \n",
    "        s,r,done,_ = env.step(a)\n",
    "        total_reward+=r\n",
    "        \n",
    "        if done or t>t_max:break\n",
    "    return states,actions,total_reward\n",
    "\n",
    "from joblib import Parallel,delayed\n",
    "generate_sessions = lambda n,n_jobs=-1: Parallel(n_jobs)(n*[delayed(generate_session)()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 \tmean reward=-249.90\tthreshold=-151.86\n",
      "epoch 1 \tmean reward=-235.98\tthreshold=-132.11\n",
      "epoch 2 \tmean reward=-218.98\tthreshold=-129.94\n",
      "epoch 3 \tmean reward=-191.62\tthreshold=-140.58\n",
      "epoch 4 \tmean reward=-159.24\tthreshold=-118.25\n",
      "epoch 5 \tmean reward=-135.37\tthreshold=-101.08\n",
      "epoch 6 \tmean reward=-164.81\tthreshold=-103.56\n",
      "epoch 7 \tmean reward=-144.51\tthreshold=-84.46\n",
      "epoch 8 \tmean reward=-137.36\tthreshold=-101.81\n",
      "epoch 9 \tmean reward=-102.69\tthreshold=-55.57\n",
      "epoch 10 \tmean reward=-94.40\tthreshold=-59.96\n",
      "epoch 11 \tmean reward=-98.63\tthreshold=-65.33\n",
      "epoch 12 \tmean reward=-89.13\tthreshold=-62.03\n",
      "epoch 13 \tmean reward=-68.67\tthreshold=-38.45\n",
      "epoch 14 \tmean reward=-71.03\tthreshold=-20.68\n",
      "epoch 15 \tmean reward=-61.06\tthreshold=-34.60\n",
      "epoch 16 \tmean reward=-46.84\tthreshold=-25.76\n",
      "epoch 17 \tmean reward=-15.51\tthreshold=-10.40\n",
      "epoch 18 \tmean reward=-21.44\tthreshold=-1.58\n",
      "epoch 19 \tmean reward=-15.27\tthreshold=-1.39\n",
      "epoch 20 \tmean reward=0.90\tthreshold=99.29\n",
      "epoch 21 \tmean reward=-9.35\tthreshold=14.02\n",
      "epoch 22 \tmean reward=5.96\tthreshold=20.57\n",
      "epoch 23 \tmean reward=2.68\tthreshold=117.85\n",
      "epoch 24 \tmean reward=-1.09\tthreshold=93.82\n",
      "epoch 25 \tmean reward=12.93\tthreshold=102.65\n",
      "epoch 26 \tmean reward=17.03\tthreshold=102.26\n",
      "epoch 27 \tmean reward=-4.65\tthreshold=13.11\n",
      "epoch 28 \tmean reward=9.07\tthreshold=66.02\n",
      "epoch 29 \tmean reward=-16.32\tthreshold=18.12\n",
      "epoch 30 \tmean reward=23.09\tthreshold=112.70\n",
      "epoch 31 \tmean reward=24.27\tthreshold=101.85\n",
      "epoch 32 \tmean reward=3.96\tthreshold=83.20\n",
      "epoch 33 \tmean reward=-2.57\tthreshold=113.13\n",
      "epoch 34 \tmean reward=9.60\tthreshold=96.01\n",
      "epoch 35 \tmean reward=14.49\tthreshold=103.08\n",
      "epoch 36 \tmean reward=8.49\tthreshold=72.21\n",
      "epoch 37 \tmean reward=-4.12\tthreshold=57.28\n",
      "epoch 38 \tmean reward=-18.17\tthreshold=56.70\n",
      "epoch 39 \tmean reward=19.15\tthreshold=85.17\n",
      "epoch 40 \tmean reward=-9.12\tthreshold=94.12\n",
      "epoch 41 \tmean reward=15.46\tthreshold=88.81\n",
      "epoch 42 \tmean reward=13.98\tthreshold=72.02\n",
      "epoch 43 \tmean reward=30.61\tthreshold=102.26\n",
      "epoch 44 \tmean reward=8.55\tthreshold=49.14\n",
      "epoch 45 \tmean reward=-19.42\tthreshold=45.36\n",
      "epoch 46 \tmean reward=18.93\tthreshold=96.11\n",
      "epoch 47 \tmean reward=-0.62\tthreshold=60.34\n",
      "epoch 48 \tmean reward=22.04\tthreshold=85.27\n",
      "epoch 49 \tmean reward=25.87\tthreshold=79.22\n",
      "epoch 50 \tmean reward=2.26\tthreshold=51.51\n",
      "epoch 51 \tmean reward=50.31\tthreshold=118.90\n",
      "epoch 52 \tmean reward=39.33\tthreshold=89.29\n",
      "epoch 53 \tmean reward=23.71\tthreshold=120.73\n",
      "epoch 54 \tmean reward=18.41\tthreshold=62.68\n",
      "epoch 55 \tmean reward=8.46\tthreshold=89.09\n",
      "epoch 56 \tmean reward=12.94\tthreshold=103.37\n",
      "epoch 57 \tmean reward=2.00\tthreshold=95.97\n",
      "epoch 58 \tmean reward=-22.65\tthreshold=51.96\n",
      "epoch 59 \tmean reward=-43.90\tthreshold=-19.03\n",
      "epoch 60 \tmean reward=12.20\tthreshold=68.36\n",
      "epoch 61 \tmean reward=-12.82\tthreshold=84.67\n",
      "epoch 62 \tmean reward=12.62\tthreshold=93.08\n",
      "epoch 63 \tmean reward=-11.15\tthreshold=70.10\n",
      "epoch 64 \tmean reward=15.27\tthreshold=111.33\n",
      "epoch 65 \tmean reward=-2.55\tthreshold=54.89\n",
      "epoch 66 \tmean reward=2.13\tthreshold=102.05\n",
      "epoch 67 \tmean reward=19.58\tthreshold=112.70\n",
      "epoch 68 \tmean reward=47.17\tthreshold=131.54\n",
      "epoch 69 \tmean reward=43.96\tthreshold=123.15\n",
      "epoch 70 \tmean reward=-44.34\tthreshold=88.98\n",
      "epoch 71 \tmean reward=-17.47\tthreshold=69.77\n",
      "epoch 72 \tmean reward=6.77\tthreshold=105.98\n",
      "epoch 73 \tmean reward=-20.23\tthreshold=64.15\n",
      "epoch 74 \tmean reward=-19.74\tthreshold=81.27\n",
      "epoch 75 \tmean reward=8.40\tthreshold=101.93\n",
      "epoch 76 \tmean reward=-0.47\tthreshold=75.65\n",
      "epoch 77 \tmean reward=7.98\tthreshold=116.75\n",
      "epoch 78 \tmean reward=24.47\tthreshold=134.21\n",
      "epoch 79 \tmean reward=29.91\tthreshold=113.50\n",
      "epoch 80 \tmean reward=-0.91\tthreshold=76.23\n",
      "epoch 81 \tmean reward=71.18\tthreshold=171.64\n",
      "epoch 82 \tmean reward=53.82\tthreshold=181.15\n",
      "epoch 83 \tmean reward=63.76\tthreshold=180.33\n",
      "epoch 84 \tmean reward=61.03\tthreshold=180.47\n",
      "epoch 85 \tmean reward=56.27\tthreshold=191.47\n",
      "epoch 86 \tmean reward=115.98\tthreshold=198.71\n",
      "epoch 87 \tmean reward=53.34\tthreshold=175.77\n",
      "epoch 88 \tmean reward=124.34\tthreshold=193.32\n",
      "epoch 89 \tmean reward=117.29\tthreshold=211.08\n",
      "epoch 90 \tmean reward=129.77\tthreshold=201.86\n",
      "epoch 91 \tmean reward=93.44\tthreshold=174.28\n",
      "epoch 92 \tmean reward=138.45\tthreshold=203.47\n",
      "epoch 93 \tmean reward=100.55\tthreshold=168.48\n",
      "epoch 94 \tmean reward=70.96\tthreshold=202.90\n",
      "epoch 95 \tmean reward=85.48\tthreshold=210.42\n",
      "epoch 96 \tmean reward=94.00\tthreshold=224.67\n",
      "epoch 97 \tmean reward=89.93\tthreshold=207.64\n",
      "epoch 98 \tmean reward=46.80\tthreshold=169.07\n",
      "epoch 99 \tmean reward=49.14\tthreshold=184.79\n",
      "epoch 100 \tmean reward=69.11\tthreshold=221.87\n",
      "epoch 101 \tmean reward=53.22\tthreshold=167.22\n",
      "epoch 102 \tmean reward=88.73\tthreshold=209.67\n",
      "epoch 103 \tmean reward=58.68\tthreshold=214.14\n",
      "epoch 104 \tmean reward=37.89\tthreshold=157.84\n",
      "epoch 105 \tmean reward=78.02\tthreshold=203.79\n",
      "epoch 106 \tmean reward=88.03\tthreshold=212.31\n",
      "epoch 107 \tmean reward=75.71\tthreshold=223.43\n",
      "epoch 108 \tmean reward=123.24\tthreshold=220.47\n",
      "epoch 109 \tmean reward=153.62\tthreshold=233.75\n",
      "epoch 110 \tmean reward=136.91\tthreshold=233.67\n",
      "epoch 111 \tmean reward=97.32\tthreshold=228.96\n",
      "epoch 112 \tmean reward=38.90\tthreshold=217.84\n",
      "epoch 113 \tmean reward=165.77\tthreshold=243.07\n",
      "epoch 114 \tmean reward=109.79\tthreshold=223.24\n",
      "epoch 115 \tmean reward=106.25\tthreshold=222.34\n",
      "epoch 116 \tmean reward=103.30\tthreshold=235.93\n",
      "epoch 117 \tmean reward=171.35\tthreshold=242.97\n",
      "epoch 118 \tmean reward=159.56\tthreshold=235.29\n",
      "epoch 119 \tmean reward=103.36\tthreshold=215.67\n",
      "epoch 120 \tmean reward=105.99\tthreshold=217.22\n",
      "epoch 121 \tmean reward=91.90\tthreshold=220.73\n",
      "epoch 122 \tmean reward=137.41\tthreshold=230.78\n",
      "epoch 123 \tmean reward=75.43\tthreshold=228.34\n",
      "epoch 124 \tmean reward=94.38\tthreshold=239.09\n",
      "epoch 125 \tmean reward=93.17\tthreshold=227.96\n",
      "epoch 126 \tmean reward=154.13\tthreshold=242.43\n",
      "epoch 127 \tmean reward=81.42\tthreshold=241.50\n",
      "epoch 128 \tmean reward=142.74\tthreshold=235.81\n",
      "epoch 129 \tmean reward=54.07\tthreshold=226.86\n",
      "epoch 130 \tmean reward=105.81\tthreshold=238.23\n",
      "epoch 131 \tmean reward=154.91\tthreshold=238.51\n",
      "epoch 132 \tmean reward=127.37\tthreshold=231.71\n",
      "epoch 133 \tmean reward=112.87\tthreshold=240.95\n",
      "epoch 134 \tmean reward=128.59\tthreshold=235.49\n",
      "epoch 135 \tmean reward=107.01\tthreshold=242.37\n",
      "epoch 136 \tmean reward=137.18\tthreshold=237.26\n",
      "epoch 137 \tmean reward=125.81\tthreshold=228.19\n",
      "epoch 138 \tmean reward=74.59\tthreshold=234.35\n",
      "epoch 139 \tmean reward=116.00\tthreshold=239.80\n",
      "epoch 140 \tmean reward=153.56\tthreshold=231.51\n",
      "epoch 141 \tmean reward=129.49\tthreshold=232.51\n",
      "epoch 142 \tmean reward=139.56\tthreshold=241.42\n",
      "epoch 143 \tmean reward=114.49\tthreshold=229.85\n",
      "epoch 144 \tmean reward=140.78\tthreshold=236.95\n",
      "epoch 145 \tmean reward=136.52\tthreshold=232.84\n",
      "epoch 146 \tmean reward=106.06\tthreshold=231.49\n",
      "epoch 147 \tmean reward=133.83\tthreshold=235.59\n",
      "epoch 148 \tmean reward=149.50\tthreshold=238.73\n",
      "epoch 149 \tmean reward=115.84\tthreshold=234.68\n"
     ]
    }
   ],
   "source": [
    "#training loop\n",
    "#if you want faster stochastic iterations, try n_samples=100,percentile=50~70. Also maybe tune learning rate.\n",
    "n_samples = 500   #takes 500 samples\n",
    "percentile = 80   #fits to 20% best (100 samples) on each epoch\n",
    "n_jobs = -1       #uses all cores\n",
    "\n",
    "\n",
    "for i in range(150):\n",
    "    #sample sessions\n",
    "    sessions = generate_sessions(n_samples,n_jobs)\n",
    "    batch_states,batch_actions,batch_rewards = map(np.array,zip(*sessions))\n",
    "    \n",
    "    #choose threshold on rewards\n",
    "    threshold = np.percentile(batch_rewards,percentile)\n",
    "    elite_states = np.concatenate(batch_states[batch_rewards>=threshold])\n",
    "    elite_actions = np.concatenate(batch_actions[batch_rewards>=threshold])\n",
    "    \n",
    "    #fit our osom neural network >.<\n",
    "    agent.fit(elite_states,elite_actions)\n",
    "\n",
    "    #report progress\n",
    "    print(\"epoch %i \\tmean reward=%.2f\\tthreshold=%.2f\"%(i,batch_rewards.mean(),threshold))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-23 05:37:28,806] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n"
     ]
    }
   ],
   "source": [
    "#finish recording\n",
    "env = gym.wrappers.Monitor(env,directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(500)]\n",
    "env.close()\n",
    "gym.upload(\"./videos/\",api_key=\"<...>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.0.24391.video000000.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
