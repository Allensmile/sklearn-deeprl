{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super-deep reinforcement learning with scikit-learn\n",
    "\n",
    "Which environment to choose for deep reinforcement learning experiments?\n",
    "Theano or Tensorflow? GPU or MPI? \n",
    "\n",
    "Who the hack needs any of these? Now you have Scikit-Learn!\n",
    "\n",
    "Jokes aside, this demo shows the awesome scikit-learn deep reinforcement learning agent trained with __crossentropy method__ _(because TD is for sissies)_.\n",
    "\n",
    "Requires: gym, scikit-learn (with MLPClassifier), numpy\n",
    "\n",
    "Read more about crossentropy method [in general](https://people.smp.uq.edu.au/DirkKroese/ps/aortut.pdf), [for rl](https://people.smp.uq.edu.au/DirkKroese/ps/eormsCE.pdf), [for rl again](https://esc.fnwi.uva.nl/thesis/centraal/files/f2110275396.pdf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-22 04:12:29,986] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "/home/jheuristic/anaconda2/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "import gym, gym.wrappers\n",
    "gym.logger.level=0 #gym.youre(\"drunk\").shut_up()\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#Create environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env = gym.wrappers.Monitor(env,directory=\"videos\",force=True)\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "\n",
    "#Create agent\n",
    "agent = MLPClassifier(hidden_layer_sizes=(20,20),\n",
    "                      activation='tanh',\n",
    "                      solver='adam',\n",
    "                      warm_start=True,max_iter=1\n",
    "                     )\n",
    "#initialize agent by feeding it with some random bullshit\n",
    "agent.fit([env.reset()]*n_actions,range(n_actions));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_session():\n",
    "    \"\"\"\n",
    "    Just ask agent to predict action and see how env reacts - repeat until exhaustion.\n",
    "    :param greedy: if True, picks most likely actions, else samples actions\"\"\"\n",
    "    states,actions,total_reward = [],[],0\n",
    "    \n",
    "    s = env.reset()    \n",
    "    while True:\n",
    "        a = np.random.choice(n_actions,p=agent.predict_proba([s])[0])\n",
    "        \n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        \n",
    "        s,r,done,_ = env.step(a)\n",
    "        total_reward+=r\n",
    "        if done:break\n",
    "        \n",
    "    return states,actions,total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 \tmean reward=33.50\tthreshold=39.00\n",
      "epoch 1 \tmean reward=38.31\tthreshold=44.00\n",
      "epoch 2 \tmean reward=42.69\tthreshold=50.60\n",
      "epoch 3 \tmean reward=48.38\tthreshold=58.30\n",
      "epoch 4 \tmean reward=47.56\tthreshold=58.00\n",
      "epoch 5 \tmean reward=53.88\tthreshold=61.30\n",
      "epoch 6 \tmean reward=56.37\tthreshold=61.30\n",
      "epoch 7 \tmean reward=60.88\tthreshold=72.60\n",
      "epoch 8 \tmean reward=70.49\tthreshold=81.00\n",
      "epoch 9 \tmean reward=91.38\tthreshold=109.80\n",
      "epoch 10 \tmean reward=96.06\tthreshold=110.30\n",
      "epoch 11 \tmean reward=114.96\tthreshold=151.30\n",
      "epoch 12 \tmean reward=142.47\tthreshold=187.60\n",
      "epoch 13 \tmean reward=134.30\tthreshold=176.30\n",
      "epoch 14 \tmean reward=154.61\tthreshold=200.00\n",
      "epoch 15 \tmean reward=174.48\tthreshold=200.00\n",
      "epoch 16 \tmean reward=185.59\tthreshold=200.00\n",
      "epoch 17 \tmean reward=188.76\tthreshold=200.00\n",
      "epoch 18 \tmean reward=193.16\tthreshold=200.00\n",
      "epoch 19 \tmean reward=189.80\tthreshold=200.00\n",
      "epoch 20 \tmean reward=195.49\tthreshold=200.00\n",
      "epoch 21 \tmean reward=193.79\tthreshold=200.00\n",
      "epoch 22 \tmean reward=194.01\tthreshold=200.00\n",
      "epoch 23 \tmean reward=195.61\tthreshold=200.00\n",
      "epoch 24 \tmean reward=198.06\tthreshold=200.00\n",
      "epoch 25 \tmean reward=197.54\tthreshold=200.00\n",
      "epoch 26 \tmean reward=198.63\tthreshold=200.00\n",
      "epoch 27 \tmean reward=197.90\tthreshold=200.00\n",
      "epoch 28 \tmean reward=198.63\tthreshold=200.00\n",
      "epoch 29 \tmean reward=198.30\tthreshold=200.00\n",
      "epoch 30 \tmean reward=200.00\tthreshold=200.00\n",
      "epoch 31 \tmean reward=199.94\tthreshold=200.00\n",
      "epoch 32 \tmean reward=200.00\tthreshold=200.00\n",
      "epoch 33 \tmean reward=198.25\tthreshold=200.00\n",
      "epoch 34 \tmean reward=200.00\tthreshold=200.00\n",
      "epoch 35 \tmean reward=199.69\tthreshold=200.00\n",
      "epoch 36 \tmean reward=199.62\tthreshold=200.00\n",
      "epoch 37 \tmean reward=200.00\tthreshold=200.00\n",
      "epoch 38 \tmean reward=200.00\tthreshold=200.00\n",
      "epoch 39 \tmean reward=200.00\tthreshold=200.00\n",
      "epoch 40 \tmean reward=199.56\tthreshold=200.00\n",
      "epoch 41 \tmean reward=200.00\tthreshold=200.00\n",
      "epoch 42 \tmean reward=197.74\tthreshold=200.00\n",
      "epoch 43 \tmean reward=200.00\tthreshold=200.00\n",
      "epoch 44 \tmean reward=200.00\tthreshold=200.00\n",
      "epoch 45 \tmean reward=198.41\tthreshold=200.00\n",
      "epoch 46 \tmean reward=198.71\tthreshold=200.00\n",
      "epoch 47 \tmean reward=199.29\tthreshold=200.00\n",
      "epoch 48 \tmean reward=199.01\tthreshold=200.00\n",
      "epoch 49 \tmean reward=199.26\tthreshold=200.00\n",
      "epoch 50 \tmean reward=197.75\tthreshold=200.00\n",
      "epoch 51 \tmean reward=199.41\tthreshold=200.00\n",
      "epoch 52 \tmean reward=200.00\tthreshold=200.00\n",
      "epoch 53 \tmean reward=200.00\tthreshold=200.00\n",
      "epoch 54 \tmean reward=199.36\tthreshold=200.00\n",
      "epoch 55 \tmean reward=199.83\tthreshold=200.00\n",
      "epoch 56 \tmean reward=200.00\tthreshold=200.00\n",
      "epoch 57 \tmean reward=199.91\tthreshold=200.00\n",
      "epoch 58 \tmean reward=199.04\tthreshold=200.00\n",
      "epoch 59 \tmean reward=199.64\tthreshold=200.00\n",
      "epoch 60 \tmean reward=199.81\tthreshold=200.00\n",
      "epoch 61 \tmean reward=199.43\tthreshold=200.00\n",
      "epoch 62 \tmean reward=199.99\tthreshold=200.00\n",
      "epoch 63 \tmean reward=199.98\tthreshold=200.00\n",
      "epoch 64 \tmean reward=199.39\tthreshold=200.00\n",
      "epoch 65 \tmean reward=198.27\tthreshold=200.00\n",
      "epoch 66 \tmean reward=199.41\tthreshold=200.00\n",
      "epoch 67 \tmean reward=199.92\tthreshold=200.00\n",
      "epoch 68 \tmean reward=200.00\tthreshold=200.00\n",
      "epoch 69 \tmean reward=200.00\tthreshold=200.00\n",
      "epoch 70 \tmean reward=200.00\tthreshold=200.00\n",
      "epoch 71 \tmean reward=198.71\tthreshold=200.00\n",
      "epoch 72 \tmean reward=199.93\tthreshold=200.00\n",
      "epoch 73 \tmean reward=200.00\tthreshold=200.00\n",
      "epoch 74 \tmean reward=200.00\tthreshold=200.00\n",
      "epoch 75 \tmean reward=198.42\tthreshold=200.00\n",
      "epoch 76 \tmean reward=200.00\tthreshold=200.00\n",
      "epoch 77 \tmean reward=200.00\tthreshold=200.00\n",
      "epoch 78 \tmean reward=200.00\tthreshold=200.00\n",
      "epoch 79 \tmean reward=200.00\tthreshold=200.00\n",
      "epoch 80 \tmean reward=200.00\tthreshold=200.00\n",
      "epoch 81 \tmean reward=200.00\tthreshold=200.00\n",
      "epoch 82 \tmean reward=200.00\tthreshold=200.00\n",
      "epoch 83 \tmean reward=199.99\tthreshold=200.00\n",
      "epoch 84 \tmean reward=199.86\tthreshold=200.00\n",
      "epoch 85 \tmean reward=199.82\tthreshold=200.00\n",
      "epoch 86 \tmean reward=199.58\tthreshold=200.00\n",
      "epoch 87 \tmean reward=198.84\tthreshold=200.00\n",
      "epoch 88 \tmean reward=199.39\tthreshold=200.00\n",
      "epoch 89 \tmean reward=199.17\tthreshold=200.00\n",
      "epoch 90 \tmean reward=199.52\tthreshold=200.00\n",
      "epoch 91 \tmean reward=199.65\tthreshold=200.00\n",
      "epoch 92 \tmean reward=199.58\tthreshold=200.00\n",
      "epoch 93 \tmean reward=199.70\tthreshold=200.00\n",
      "epoch 94 \tmean reward=199.48\tthreshold=200.00\n",
      "epoch 95 \tmean reward=199.98\tthreshold=200.00\n",
      "epoch 96 \tmean reward=198.25\tthreshold=200.00\n",
      "epoch 97 \tmean reward=200.00\tthreshold=200.00\n",
      "epoch 98 \tmean reward=200.00\tthreshold=200.00\n",
      "epoch 99 \tmean reward=200.00\tthreshold=200.00\n"
     ]
    }
   ],
   "source": [
    "#training loop\n",
    "n_samples = 100 #take 100 samples\n",
    "percentile = 70 #fit on top 30% (30 best samples)\n",
    "\n",
    "for i in range(50):\n",
    "    #sample sessions\n",
    "    sessions = [generate_session() for _ in range(n_samples)]\n",
    "    batch_states,batch_actions,batch_rewards = map(np.array,zip(*sessions))\n",
    "    \n",
    "    #choose threshold on rewards\n",
    "    threshold = np.percentile(batch_rewards,percentile)\n",
    "    elite_states = np.concatenate(batch_states[batch_rewards>=threshold])\n",
    "    elite_actions = np.concatenate(batch_actions[batch_rewards>=threshold])\n",
    "    \n",
    "    #fit our osom neural network >.<\n",
    "    agent.fit(elite_states,elite_actions)\n",
    "\n",
    "    #report progress\n",
    "    print(\"epoch %i \\tmean reward=%.2f\\tthreshold=%.2f\"%(i,batch_rewards.mean(),threshold))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#finish recording\n",
    "env.close()\n",
    "gym.upload(\"./videos/\",api_key=\"<...>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.0.4649.video006000.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
